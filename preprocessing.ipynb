{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import networkx as nx # the main libary we will use\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from scipy import sparse\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file_object):\n",
    "    \"\"\"Lazy function (generator) to read a file piece by piece. \"\"\"\n",
    "    while True:\n",
    "        data = file_object.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(new_file_name):\n",
    "    \n",
    "    # Create the file\n",
    "    with open(new_file_name, 'w', encoding='utf-8') as f:\n",
    "        pass\n",
    "    with open(new_file_name, 'a', encoding='utf-8') as db:\n",
    "        with open('dblp.v12.json') as f:\n",
    "            for piece in tqdm(read_in_chunks(f)):\n",
    "                try:\n",
    "                    if json.loads(piece[1:])['year']>=2010: \n",
    "                        db.write(piece[1:])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Error:\", e)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1 ####\n",
    "1. Download dataset from here https://www.kaggle.com/mathurinache/citation-network-dataset \n",
    "and unzip it to the same directory as this jupyter file\n",
    "2. run next cell to make a subset of this dataset called '2010_2020_db.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fb54adc2634f2bb7a2e72c0946d35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting value: line 2 column 1 (char 1)\n",
      "Error: Extra data: line 1 column 5 (char 4)\n",
      "Error: Expecting value: line 2 column 1 (char 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_file_name = '2010_2020_db.json'\n",
    "\n",
    "if new_file_name in os.listdir():\n",
    "    pass\n",
    "else:\n",
    "    parse_file(new_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2 ###\n",
    " create a csv file with data of edges, years and weights called '2010_2020_weighted_edges.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edges(piece, return_=False):\n",
    "    global edges_by_year\n",
    "    \n",
    "    nodes_lst = [auth['id'] for auth in json.loads(piece)['authors']]\n",
    "    year = json.loads(piece)['year']\n",
    "    if year in edges_by_year.keys():\n",
    "        edges_by_year[year] += list(itertools.combinations(nodes_lst, 2))\n",
    "    else:\n",
    "        edges_by_year[year] = list(itertools.combinations(nodes_lst, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edges_by_year(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        for piece in tqdm(read_in_chunks(f)):\n",
    "            try:\n",
    "                generate_edges(piece)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee4687a7a1a4f6896d11c2a6514eec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "Error: 'authors'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_by_year = dict()\n",
    "\n",
    "file_name = '2010_2020_db.json'\n",
    "\n",
    "generate_edges_by_year(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_edges = []\n",
    "for key in edges_by_year.keys():\n",
    "    sorted_pairs = [sorted(pair) for pair in edges_by_year[key]]\n",
    "    total_edges+= [(x[0], x[1], key) for x in sorted_pairs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = pd.DataFrame(total_edges, columns=['Target', 'Source', 'year']).groupby(['Target', 'Source', 'year'], \n",
    "                                                                                 as_index=False).size()\n",
    "\n",
    "df_edges = df_edges.reset_index()\n",
    "df_edges.columns = [*df_edges.columns.values[:-1], 'weight']\n",
    "df_edges = df_edges.sort_values(by=['year','weight'], ascending=False)\n",
    "\n",
    "df_edges.to_csv('2010_2020_weighted_edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3 ###\n",
    "create a csv file called '2019_authors_names.csv' with author_id and the following info:\n",
    "1. author_name \n",
    "2. author number of papers\n",
    "3. author number of collaborations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_authors_dict(new_file_name):\n",
    "    auth_names = dict()\n",
    "    auth_num_collab = dict()\n",
    "    auth_num_papers = dict()\n",
    "    with open(new_file_name, 'r', encoding='utf-8') as f:\n",
    "        for piece in tqdm(read_in_chunks(f)):\n",
    "            try:\n",
    "                if json.loads(piece)['year'] == 2019:\n",
    "                    num_collab = len(json.loads(piece)['authors'])-1\n",
    "                    for author in json.loads(piece)['authors']:\n",
    "                        if author['id'] not in auth_names.keys():\n",
    "                            auth_names[author['id']] = author['name']\n",
    "                            auth_num_collab[author['id']] = num_collab\n",
    "                            auth_num_papers[author['id']] = 1\n",
    "                        else:\n",
    "                            auth_num_collab[author['id']] += max(0,num_collab)\n",
    "                            auth_num_papers[author['id']] += 1\n",
    "                                                       \n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "            \n",
    "    return auth_names, auth_num_collab, auth_num_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3603828ffc4bc99ed17b648929fd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = '2010_2020_db.json'\n",
    "auth_names, auth_num_collab, auth_num_papers = create_authors_dict(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.DataFrame(list(auth_names.items()),columns = ['author_id','author_name'])\n",
    "df_papers = pd.DataFrame(list(auth_num_papers.items()),columns = ['author_id','num_papers'])\n",
    "df_collabs = pd.DataFrame(list(auth_num_collab.items()),columns = ['author_id','num_collabs'])\n",
    "\n",
    "df_names = df_names.merge(df_papers, how='left', on='author_id')\n",
    "df_names = df_names.merge(df_collabs, how='left', on='author_id')\n",
    "\n",
    "df_names.to_csv('2019_authors_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4 ###\n",
    "generating the data for Tel Aviv university case study\n",
    "1. find nodes that belong to tlv university\n",
    "2. parse 'org' field of each node\n",
    "3. filter only edges that include tlv reserches and create a csv file called 'tlv_edges.csv'\n",
    "4. create 'tlv_nodes.csv'\n",
    "5. create edges and nodes for the following:\n",
    "    1. industrial engineering network\n",
    "    2. top 5 authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auth_edges = pd.read_csv('2010_2020_weighted_edges.csv')\n",
    "df_auth_edges['Target'] = df_auth_edges['Target'].astype('str')\n",
    "df_auth_edges['Source'] = df_auth_edges['Source'].astype('str')\n",
    "\n",
    "auth_names = pd.read_csv('2019_authors_names.csv')[['author_id', 'author_name']]\n",
    "auth_names['author_id'] = auth_names['author_id'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## please make sure you run the second preprocessing file before running next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_org = pd.read_csv('2019_authors_org.csv')\n",
    "auth_org['author_id'] = auth_org['author_id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_org.drop(columns=['org_list', 'most_common'], inplace=True)\n",
    "auth_org['common_alias'] = auth_org['common_alias'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlv_auth = auth_org[(auth_org.common_alias.str.contains('tel-aviv university', regex=False))|\n",
    "                    (auth_org.common_alias.str.contains('tel aviv university', regex=False))]\n",
    "# tlv_auth.to_csv('2019_tel_aviv_authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlv_departments = ['school of medicine', 'school of computer science', 'coller school of management',\n",
    "                   'department of industrial engineering', 'department of mathematics', 'faculty of life sciences', \n",
    "                   'department of archaeology and ancient near eastern civilizations', \n",
    "                   'department of biomedical engineering','department of disaster management and injury prevention', \n",
    "                   'department of ee-systems', 'department of electrical engineering',\n",
    "                   'department of geography and human environment',\n",
    "                  'department of neurobiology', 'department of physical therapy', \n",
    "                   'department of statistics and operations research', 'department of sociology and anthropology',\n",
    "                  'faculty of engineering', 'school of neuroscience', \n",
    "                   'porter school of environment and earth sciences', 'school of economics', 'school of education',\n",
    "                   'school of mechanical engineering', 'school of physics and astronomy', \n",
    "                   'school of plant sciences and food security', 'school of political science',\n",
    "                   'school of psychological sciences', \n",
    "                   'sackler center for computational molecularand materials science'\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, analyzer='char', ngram_range=(3, 3))\n",
    "sample = list(tlv_auth.common_alias.values)+ tlv_departments\n",
    "sample = np.array(sample)\n",
    "\n",
    "org_vectorized = vectorizer.fit_transform(sample)\n",
    "results = []\n",
    "for i in range(len(tlv_auth)):\n",
    "    cosine_similarities = linear_kernel(org_vectorized[i:i+1], org_vectorized[len(tlv_auth):])\n",
    "    related_docs = cosine_similarities.argsort()\n",
    "    \n",
    "    if cosine_similarities[0][related_docs[0][-1]]< 0.17:\n",
    "        parsed_org = 'tel aviv university'\n",
    "    else:\n",
    "        parsed_org = tlv_departments[related_docs[0][-1]]\n",
    "        \n",
    "    results.append(tuple((tlv_auth.common_alias.values[i], \n",
    "                          parsed_org, \n",
    "                          cosine_similarities[0][related_docs[0][-1]])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(results, columns=['common_alias', 'org', 'score']).to_csv(\"parsed_org.csv\", index=False)\n",
    "\n",
    "# after this file is generated, We also made manual debug and saved it in a new file called 'manual_parsed_org.csv'\n",
    "\n",
    "if 'manual_parsed_org.csv' in os.listdir():\n",
    "    df_parsed_org = pd.read_csv(\"manual_parsed_org.csv\")\n",
    "    print('manual')\n",
    "else:\n",
    "    df_parsed_org = pd.read_csv(\"parsed_org.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlv_auth = tlv_auth.merge(df_parsed_org[['common_alias', 'org']], how='left', on='common_alias')\n",
    "tlv_auth.drop(columns='common_alias', inplace=True)\n",
    "tlv_auth.columns = ['author_id', 'common_alias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_per_id = {'2306984392': 'department of statistics and operations research',\n",
    "              '2070965055': 'department of industrial engineering',\n",
    "              '2149966762': 'department of electrical engineering', \n",
    "              '2980878239': 'school of computer science', '9988965': 'school of computer science',\n",
    "              '687553606': 'school of computer science', '1484279603': 'school of computer science',\n",
    "              '2775315467': 'school of education', '1345056057': 'school of computer science',\n",
    "              '2144636783': 'school of computer science','291038320': 'department of mathematics',\n",
    "              '2119879193': 'department of mathematics', '2918170205': 'school of computer science',\n",
    "              '1577021311': 'school of computer science','2926313978': 'school of computer science',\n",
    "              '2263326892': 'school of computer science','2128632650': 'school of computer science',\n",
    "              '2792358084': 'school of mechanical engineering', '2796515955': 'school of computer science',\n",
    "              '2078633956': 'school of computer science','1983463915': 'school of computer science',\n",
    "              '2896050041': 'school of computer science', '2737322503' : 'school of computer science',\n",
    "              '1931272019' : 'department of electrical engineering', '2307885315' : 'school of computer science',\n",
    "              '2566053256' : 'school of computer science','2962584599' : 'department of electrical engineering', \n",
    "              '2961594172' : 'department of electrical engineering', '2153205782' : 'school of computer science',\n",
    "              '2963245526' : 'school of computer science', '2963650871' : 'school of computer science',\n",
    "              '564112320' : 'department of electrical engineering', '17138338': 'department of mathematics',\n",
    "              '2570290309' : 'school of computer science', '2946412730' : 'school of computer science',\n",
    "              '2230089554' : 'school of computer science', '2969834779' : 'school of computer science',\n",
    "              '2588517499': 'department of industrial engineering', '2499484578' : 'school of computer science',\n",
    "              '349649168' : 'school of computer science', '2973505757' : 'school of computer science',\n",
    "              '2553854373' : 'school of computer science', '2121423365' : 'school of computer science',\n",
    "              '129682478' : 'school of computer science', '2982013605': 'department of industrial engineering', \n",
    "              '2985243338': 'school of computer science', '2769654274': 'school of computer science',\n",
    "              '2229613137': 'school of computer science', '2998807896': 'school of computer science',\n",
    "              '3003321027': 'school of computer science', '53423': 'school of computer science',\n",
    "              '9985198': 'department of electrical engineering', '10173300': 'department of electrical engineering',\n",
    "             '216695685': 'school of computer science', '281847691': 'department of industrial engineering',\n",
    "             '839208152': 'department of electrical engineering', '1479935207': 'school of computer science',\n",
    "             '1534604502': 'Massachusetts Institute of Technology', \n",
    "             '1673554890' :'department of electrical engineering', '2080905167': 'school of computer science',\n",
    "             '2158063030': 'school of computer science', '2797071518': 'school of medicine',\n",
    "             '2945866313': 'school of computer science', '2230954642': 'school of computer science',\n",
    "             '2538031590': 'school of computer science', '2785819771': 'school of computer science',\n",
    "             '2986656312': 'school of computer science', '2151425129': 'school of computer science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_manual_aff = pd.DataFrame(org_per_id.items(), columns=['author_id', 'common_alias'])\n",
    "auth_manual_aff['author_id'] = auth_manual_aff['author_id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlv_auth = pd.concat([tlv_auth[~tlv_auth.author_id.isin(auth_manual_aff.author_id)],\n",
    "                     auth_manual_aff])\n",
    "tlv_auth = tlv_auth.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlv_auth_co_2019 = df_auth_edges[df_auth_edges.year==2019]\n",
    "\n",
    "tlv_auth_co_2019 = tlv_auth_co_2019[(tlv_auth_co_2019.Target.isin(tlv_auth.author_id))|\n",
    "                                    (tlv_auth_co_2019.Source.isin(tlv_auth.author_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlv_auth_co_2019 = tlv_auth_co_2019.merge(tlv_auth, how='left', left_on='Target', right_on='author_id')\n",
    "tlv_auth_co_2019 = tlv_auth_co_2019.merge(auth_org, how='left', left_on='Target', right_on='author_id')\n",
    "tlv_auth_co_2019['target_org'] = tlv_auth_co_2019[['common_alias_x', 'common_alias_y']\n",
    "                                                 ].apply(lambda x: x[1] if pd.isna(x[0]) else x[0], axis=1)\n",
    "\n",
    "tlv_auth_co_2019.drop(columns = ['common_alias_x', 'common_alias_y'], inplace=True)\n",
    "\n",
    "tlv_auth_co_2019 = tlv_auth_co_2019.merge(tlv_auth, how='left', left_on='Source', right_on='author_id')\n",
    "tlv_auth_co_2019 = tlv_auth_co_2019.merge(auth_org, how='left', left_on='Source', right_on='author_id')\n",
    "tlv_auth_co_2019['source_org'] = tlv_auth_co_2019[['common_alias_x', 'common_alias_y']\n",
    "                                                 ].apply(lambda x: x[1] if pd.isna(x[0]) else x[0], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "tlv_auth_co_2019 = tlv_auth_co_2019.merge(auth_names, how='left', left_on='Target', right_on='author_id')\n",
    "tlv_auth_co_2019 = tlv_auth_co_2019.merge(auth_names, how='left', left_on='Source', right_on='author_id')\n",
    "\n",
    "tlv_auth_co_2019.drop(columns = ['common_alias_x', 'common_alias_y', 'author_id_x', 'author_id_y'], inplace=True)\n",
    "\n",
    "tlv_auth_co_2019.columns = ['Target', 'Source', 'year', 'weight', 'target_org', 'source_org', \n",
    "                        'target_name', 'source_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlv_auth_co_2019[['Target', 'Source', 'weight']].to_csv('tlv_edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = tlv_auth_co_2019[['Target', 'target_name', 'target_org']]\n",
    "df1.columns = ['id','label','org']\n",
    "df2 = tlv_auth_co_2019[['Source', 'source_name', 'source_org']]\n",
    "df2.columns = ['id','label','org']\n",
    "\n",
    "nodes4gephi = pd.concat([df1, df2]).drop_duplicates()\n",
    "\n",
    "nodes4gephi = nodes4gephi.replace(['tel aviv univ, israel', 'tel‚Äêaviv university ,', 'tel aviv univesity', \n",
    "                                   'tel aviv univesity, tel aviv, israel'], 'tel aviv university')\n",
    "\n",
    "nodes4gephi.to_csv('tlv_nodes.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph(list_of_roots, forest, depth_limit=3):\n",
    "    for root in list_of_roots:\n",
    "        forest.append([sorted(e) for e in nx.dfs_edges(g_tlv, source=int(root), depth_limit = depth_limit)])\n",
    "    \n",
    "    return forest\n",
    "\n",
    "def get_nodes_and_edges_files(subgraph_df, files_names):\n",
    "    \n",
    "    subgraph_df[['Target', 'Source', 'weight']].to_csv(files_names[0], index=False)\n",
    "    \n",
    "    df1 = subgraph_df[['Target', 'target_name', 'target_org']]\n",
    "    df1.columns = ['id','label','org']\n",
    "    df2 = subgraph_df[['Source', 'source_name', 'source_org']]\n",
    "    df2.columns = ['id','label','org']\n",
    "    \n",
    "    nodes4gephi = pd.concat([df1, df2]).drop_duplicates()\n",
    "    nodes4gephi = nodes4gephi.replace(['tel aviv univ, israel', 'tel‚Äêaviv university ,', 'tel aviv univesity', \n",
    "                                       'tel aviv univesity, tel aviv, israel'], 'tel aviv university')\n",
    "    \n",
    "    nodes4gephi.to_csv(files_names[1], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "industrial_nodes = set()\n",
    "industrial_nodes.update(\n",
    "    tlv_auth_co_2019[tlv_auth_co_2019['target_org'] == 'department of industrial engineering'].Target.values)\n",
    "\n",
    "industrial_nodes.update(\n",
    "    tlv_auth_co_2019[tlv_auth_co_2019['source_org'] == 'department of industrial engineering'].Source.values)\n",
    "\n",
    "indust_roots = list(industrial_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create industrial Enginnering network ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = get_subgraph(indust_roots, [], depth_limit=None)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(d, columns=['Target', 'Source']) for d in forest])\n",
    "df['Target'] = df['Target'].astype('str')\n",
    "df['Source'] = df['Source'].astype('str')\n",
    "\n",
    "subgraph_df = pd.merge(df, tlv_auth_co_2019, on=['Target', 'Source'])\n",
    "get_nodes_and_edges_files(subgraph_df, ['indst_edges4gephi.csv', 'indst_nodes4gephi.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create TLV top5 Authors network ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = get_subgraph([9988965, 580757126, 2294996757, 2059111593, 1345056057], [], depth_limit=5)\n",
    "df = pd.concat([pd.DataFrame(d, columns=['Target', 'Source']) for d in forest])\n",
    "df['Target'] = df['Target'].astype('str')\n",
    "df['Source'] = df['Source'].astype('str')\n",
    "\n",
    "subgraph_df = pd.merge(df, tlv_auth_co_2019, on=['Target', 'Source'])\n",
    "\n",
    "get_nodes_and_edges_files(subgraph_df, ['top5_edges4gephi.csv', 'top5_nodes4gephi.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5 ###\n",
    "\n",
    "Generating universities data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_2019():   \n",
    "    # Create the file\n",
    "    with open('2019_db.json', 'w', encoding='utf-8') as f:\n",
    "#         print('hey1')\n",
    "        pass\n",
    "    with open('2019_db.json', 'a', encoding='utf-8') as db:\n",
    "        with open('dblp.v12.json',encoding='utf-8') as f:\n",
    "            for piece in read_in_chunks(f):\n",
    "                try:\n",
    "#                     print('hey2')\n",
    "                    if json.loads(piece[1:])['year']==2019: \n",
    "                        db.write(piece[1:])\n",
    "#                         print('hey3')\n",
    "                except Exception as e:\n",
    "                    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting value: line 2 column 1 (char 1)\n",
      "Error: Extra data: line 1 column 5 (char 4)\n",
      "Error: Expecting value: line 2 column 1 (char 1)\n"
     ]
    }
   ],
   "source": [
    "parse_file_2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(lambda: set())\n",
    "d_len = {}\n",
    "\n",
    "with open('2019_db.json', 'r', encoding='utf-8') as f:\n",
    "    for piece in read_in_chunks(f):\n",
    "        try:\n",
    "            for author in json.loads(piece)['authors']:\n",
    "                if('org' not in author): continue\n",
    "                auth_id = author['id']\n",
    "                d[auth_id].add(author['org'])\n",
    "                d_len[auth_id] = len(d[auth_id])\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = pd.DataFrame(list(d.items()),columns = ['author_id','org_list']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org_counter = pd.DataFrame(list(d_len.items()),columns = ['author_id','org_number']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>org_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>2150708589</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26772</th>\n",
       "      <td>2109386830</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14180</th>\n",
       "      <td>2104129307</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8350</th>\n",
       "      <td>2097525001</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2136229164</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207003</th>\n",
       "      <td>2937601747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207002</th>\n",
       "      <td>2939141111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207000</th>\n",
       "      <td>2135445625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206999</th>\n",
       "      <td>2796705657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495384</th>\n",
       "      <td>2976123406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495385 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author_id  org_number\n",
       "4254    2150708589          59\n",
       "26772   2109386830          46\n",
       "14180   2104129307          46\n",
       "8350    2097525001          43\n",
       "1090    2136229164          42\n",
       "...            ...         ...\n",
       "207003  2937601747           1\n",
       "207002  2939141111           1\n",
       "207000  2135445625           1\n",
       "206999  2796705657           1\n",
       "495384  2976123406           1\n",
       "\n",
       "[495385 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_org_counter.sort_values(by='org_number',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg = pd.merge(df_org_counter,df_org, on=['author_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "    return occurence_count.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg['most_common'] = mrg.org_list.apply(lambda x : most_frequent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases = {}\n",
    "for idx,row in mrg.iterrows():\n",
    "    for org in row.org_list:\n",
    "        aliases[org] = row.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg['common_alias'] = mrg.most_common.apply(lambda x : aliases[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_author_most_common = dict(zip(mrg.author_id, mrg.common_alias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tup = []\n",
    "with open('2019_db.json', 'r', encoding='utf-8') as f:\n",
    "    for piece in read_in_chunks(f):\n",
    "        try:\n",
    "            nodes_lst = [auth['id'] for auth in json.loads(piece)['authors']]\n",
    "            nodes_exist = set()\n",
    "            for j in nodes_lst:\n",
    "                if(j in dict_author_most_common):\n",
    "                    nodes_exist.add(dict_author_most_common[j])\n",
    "            if(len(nodes_exist)>1):\n",
    "                list_tup += list(itertools.combinations(nodes_exist, 2))                 \n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tup = [sorted(pair) for pair in list_tup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(x):\n",
    "    x1 = x[1].lower().replace('university','').replace('#tab#','').replace('#n#','').replace('department','').replace('institute','').replace('technology','').replace('of','')\n",
    "    x2 = x[2].lower().replace('university','').replace('#tab#','').replace('#n#','').replace('department','').replace('institute','').replace('technology','').replace('of','')\n",
    "    return SequenceMatcher(None, x1, x2).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>org1</th>\n",
       "      <th>org2</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303029</th>\n",
       "      <td>303029</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>GOOGLE BRAIN</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471792</th>\n",
       "      <td>471792</td>\n",
       "      <td>Tsinghua Univ., China</td>\n",
       "      <td>Tsinghua University,</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175180</th>\n",
       "      <td>175180</td>\n",
       "      <td>Department of Computer Science and Engineering...</td>\n",
       "      <td>Shanghai Jiao Tong university,</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303062</th>\n",
       "      <td>303062</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>Google Research,</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459420</th>\n",
       "      <td>459420</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>Stanford, University</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186017</th>\n",
       "      <td>186017</td>\n",
       "      <td>Department of Computer Science, University of ...</td>\n",
       "      <td>Fudan University</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186016</th>\n",
       "      <td>186016</td>\n",
       "      <td>Department of Computer Science, University of ...</td>\n",
       "      <td>Imperial College, London #N#United Kingdom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186015</th>\n",
       "      <td>186015</td>\n",
       "      <td>Department of Computer Science, University of ...</td>\n",
       "      <td>Department of Mathematics University of Warwick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186014</th>\n",
       "      <td>186014</td>\n",
       "      <td>Department of Computer Science, University of ...</td>\n",
       "      <td>École Polytechnique de Montréal Montréal Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492788</th>\n",
       "      <td>492788</td>\n",
       "      <td>メディカル・インフォメーションセンター</td>\n",
       "      <td>先端情報・通信機構学</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492789 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                               org1  \\\n",
       "303029  303029                                             GOOGLE   \n",
       "471792  471792                              Tsinghua Univ., China   \n",
       "175180  175180  Department of Computer Science and Engineering...   \n",
       "303062  303062                                             GOOGLE   \n",
       "459420  459420                                           Stanford   \n",
       "...        ...                                                ...   \n",
       "186017  186017  Department of Computer Science, University of ...   \n",
       "186016  186016  Department of Computer Science, University of ...   \n",
       "186015  186015  Department of Computer Science, University of ...   \n",
       "186014  186014  Department of Computer Science, University of ...   \n",
       "492788  492788                                メディカル・インフォメーションセンター   \n",
       "\n",
       "                                                   org2  weight  \n",
       "303029                                     GOOGLE BRAIN      94  \n",
       "471792                             Tsinghua University,      87  \n",
       "175180                   Shanghai Jiao Tong university,      77  \n",
       "303062                                 Google Research,      71  \n",
       "459420                             Stanford, University      70  \n",
       "...                                                 ...     ...  \n",
       "186017                                 Fudan University       1  \n",
       "186016       Imperial College, London #N#United Kingdom       1  \n",
       "186015  Department of Mathematics University of Warwick       1  \n",
       "186014  École Polytechnique de Montréal Montréal Canada       1  \n",
       "492788                                       先端情報・通信機構学       1  \n",
       "\n",
       "[492789 rows x 4 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges = pd.DataFrame(list_tup, columns=['org1', 'org2']).groupby(['org1', 'org2'], as_index=False).size()\n",
    "df_edges = df_edges.reset_index()\n",
    "df_edges.columns = [*df_edges.columns.values[:-1], 'weight']\n",
    "df_edges.sort_values(by='weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['sim'] = df_edges.apply(similar,axis=1)\n",
    "df_edges = df_edges[(df_edges.sim<0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.to_csv('2019_uni_weighted_edges_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
